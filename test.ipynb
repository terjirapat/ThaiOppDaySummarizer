{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a003720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to help! What would you like me to do? Do you have a specific question, topic, or activity in mind? Let me know and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"llama3\", messages=[{\"role\": \"user\", \"content\": \"prompt\"}])\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dd3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from abc import ABC, abstractmethod\n",
    "from ollama import Client  # Make sure to: `pip install ollama`\n",
    "\n",
    "class LLMChatInterface(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def UserMessage(self, text: str) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def AIMessage(self, text: str) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def SystemMessage(self, text: str) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, system_prompt: str, messages: List[Dict[str, Any]]) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class OllamaLocalChat(LLMChatInterface):\n",
    "    def __init__(self, model_name: str = \"llama3\", temperature: float = 0):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.client = Client(host=\"http://localhost:11434\")  # Ollama default\n",
    "\n",
    "    def UserMessage(self, text: str) -> Dict[str, Any]:\n",
    "        return {\"role\": \"user\", \"content\": text}\n",
    "\n",
    "    def AIMessage(self, text: str) -> Dict[str, Any]:\n",
    "        return {\"role\": \"assistant\", \"content\": text}\n",
    "\n",
    "    def SystemMessage(self, text: str) -> Dict[str, Any]:\n",
    "        return {\"role\": \"system\", \"content\": text}\n",
    "\n",
    "    def run(self, system_prompt: str, messages: List[Dict[str, Any]]) -> str:\n",
    "        all_messages = [self.SystemMessage(system_prompt)] + messages\n",
    "\n",
    "        response = self.client.chat(\n",
    "            model=self.model_name,\n",
    "            messages=all_messages,\n",
    "            options={\"temperature\": self.temperature}\n",
    "        )\n",
    "\n",
    "        return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Assistant: I'm happy to assist you in any way I can. Whether it's answering questions, providing information, or just chatting, I'm here for you.\n",
      "\n",
      "If you're looking for something specific, feel free to ask me anything. If not, we could play a game, have a fun conversation, or even generate some creative ideas together!\n",
      "\n",
      "What do you say?\n",
      "Assistant: Hi again! It's great to chat with you. I'm here to listen and help in any way I can.\n",
      "\n",
      "If you're feeling stuck or need advice on something, just let me know what's been going on and how I can assist you. Or if you'd like to talk about a particular topic, such as hobbies, interests, or goals, I'm happy to explore that with you.\n",
      "\n",
      "We could also play a game, generate some creative ideas, or even have a fun conversation! What sounds appealing to you today?\n",
      "Assistant: That sounds like so much fun! I think it would be really cool to play a game. Do you have any favorites or are you open to suggestions?\n",
      "\n",
      "I can offer a few options:\n",
      "\n",
      "1. Would You Rather: I'll give you two options, and you choose which one you prefer.\n",
      "2. Two Truths and a Lie: You tell me three statements about yourself, but one of them is false. I'll try to guess which one it is!\n",
      "3. Hangman: We can play a game of hangman where I think of a word or phrase and you try to guess it by suggesting letters.\n",
      "4. Word Chain: We take turns saying a word that starts with the last letter of the previous word.\n",
      "\n",
      "Which one catches your eye?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def run_llama3(messages):\n",
    "    response = ollama.chat(model=\"llama3\", messages=messages)\n",
    "    print(\"Assistant:\", response[\"message\"][\"content\"])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while True:\n",
    "    input_prompt = input(\">>> \")\n",
    "    if input_prompt == 'exit':\n",
    "        break\n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": input_prompt})\n",
    "    # Get AI response\n",
    "    ai_response = run_llama3(messages)\n",
    "    # Add AI message to conversation\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
