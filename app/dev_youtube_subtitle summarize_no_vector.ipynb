{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91c6e4",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/687d132f-70f0-8003-8800-16c45376f179"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92157c8c",
   "metadata": {},
   "source": [
    "i extract subtitle from youtube video\n",
    "the video is 2hours long\n",
    "i want summarize with Key points of the video using llm model\n",
    "so i chunk subtitle every 10 minutes and input each chunk to model to summarize\n",
    "eg. 1 time summarize for 1 chunk\n",
    "and then I combine all the sumarize together\n",
    "I use this method because llm model seem to cunfuse when I input the large amount of data into it\n",
    "so I chunk it first and do all above\n",
    "is it a good method or any method suggest that better than this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc77229",
   "metadata": {},
   "source": [
    "to do\n",
    "- try chunking in token \n",
    "    - 1,000–2,000 tokens per chunk is the safe sweet spot\n",
    "    - Add 10–20% overlap to catch mid-topic cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab60894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=ZCoK9nakeJE\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "title = soup.title.string.replace(\" - YouTube\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713eedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get video title name\n",
    "def clean_title(text):\n",
    "    return re.sub(r'[^0-9a-zA-Z\\u0E00-\\u0E7F\\.]', '', text)\n",
    "title = clean_title(text=title)\n",
    "\n",
    "# get video id\n",
    "match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", url)\n",
    "if match:\n",
    "    video_id = match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0689e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Fetch transcript (auto-captions or uploaded)\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['th', 'en'])\n",
    "\n",
    "# Optionally, save to file\n",
    "file_name = f'{title}_{video_id}'\n",
    "with open(f\"{file_name}_subtitle.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in transcript:\n",
    "        f.write(f\"{entry['start']:.2f}s: {entry['text']}\\n\")\n",
    "        # f.write(f\"{entry['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec099dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_subtitles(file_path):\n",
    "    subtitles = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.match(r'([0-9.]+)s:\\s(.+)', line.strip())\n",
    "            if match:\n",
    "                start_time = float(match.group(1))\n",
    "                text = match.group(2)\n",
    "                subtitles.append({'start': start_time, 'text': text})\n",
    "    return subtitles\n",
    "\n",
    "def chunk_subtitles(subtitles, chunk_size=60, overlap=20):\n",
    "    \"\"\"\n",
    "    Chunk subtitles into segments of `chunk_size` seconds with `overlap` seconds.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    max_time = subtitles[-1]['start']\n",
    "    start_time = 0\n",
    "\n",
    "    while start_time <= max_time:\n",
    "        end_time = start_time + chunk_size\n",
    "        chunk_text = []\n",
    "        for entry in subtitles:\n",
    "            if start_time <= entry['start'] < end_time:\n",
    "                chunk_text.append(entry['text'])\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                'start': start_time,\n",
    "                'end': end_time,\n",
    "                'text': ' '.join(chunk_text)\n",
    "            })\n",
    "        start_time += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "subtitles = load_subtitles(f\"{file_name}_subtitle.txt\")\n",
    "chunk_dict = chunk_subtitles(subtitles, chunk_size=600, overlap=60)\n",
    "# chunk_dict = chunk_subtitles(subtitles, chunk_size=300, overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c3f41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def read_text_file(file_path):\n",
    "#     \"\"\"Read text from a file and return as a string.\"\"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         return f.read()\n",
    "\n",
    "# def remove_timestamps(text):\n",
    "#     \"\"\"\n",
    "#     Remove timestamps at the start of each line, e.g., '0.12s: '\n",
    "#     \"\"\"\n",
    "#     # Remove pattern: start of line, optional whitespace, numbers, dot, numbers, 's:', optional whitespace\n",
    "#     cleaned_text = re.sub(r'^\\s*\\d+\\.\\d+s:\\s*', '', text, flags=re.MULTILINE)\n",
    "#     return cleaned_text\n",
    "\n",
    "# def chunk_text(text, max_tokens, overlap):\n",
    "#     \"\"\"\n",
    "#     Chunk text into segments with a max token limit and overlap.\n",
    "#     \"\"\"\n",
    "#     words = text.split()\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "\n",
    "#     while start < len(words):\n",
    "#         end = min(start + max_tokens, len(words))\n",
    "#         chunk = ' '.join(words[start:end])\n",
    "#         chunks.append(chunk)\n",
    "#         start += max_tokens - overlap  # slide window with overlap\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# # Example usage:\n",
    "# file_path = f\"{file_name}_subtitle.txt\"\n",
    "# text = read_text_file(file_path)\n",
    "# cleaned_text = remove_timestamps(text)\n",
    "\n",
    "# chunks = chunk_text(cleaned_text, max_tokens=200, overlap=60)\n",
    "\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     print(f\"Chunk {i+1}:\\n{chunk}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e881d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'end': 600,\n",
       " 'text': 'หุ้น DCC ขอมาก็จัดให้สวัสดีครับผมผิหวัง นะครับคุณกัลฟังพcสนี้เป็นล้านเลยเหรอพี่  Now นะครับอัปเดตทุกสถานการณ์การลงทุน ที่ส่งผลกับเงินเบาของทุกคนนะครับผมคลิป นี้ตามสั่งครับตามรีquestเลยครับคือใน ช่องผมนี่ก็จะมีทำเรื่องเกี่ยวกับหุ้น ประมาณว่าแบบเป็นหุ้น Value หุ้นที่มี ราคาค่อนข้างถูกถ้าเทียบกับ PE หรือว่า ค่าต่างๆนะครับหรือแม้กระทั่งหุ้นปันผล นอกเหนือจากหุ้นโกดหุ้นเติบโตหุ้นที่เรา มักจะได้ยินชื่อหุ้นหุ้นที่มีกระแสหรือ ว่าหุ้นที่คิดว่านักลงทุนเพื่อนๆจะได้ ความรู้ครับก็จะมีหุ้นประเภท Value เหมือนกันแล้วจริงๆอ่ะชื่อหุ้นเนี้ยมันมา เป็นพักๆครับมันไม่ได้มาบ่อยก็คือแบบผ่าน ไปสักพักก็จะโผล่มาคอมเมนต์นึงขอถึงหุ้น ตัวนี้ครับแล้วช่วงนี้ก็เป็นช่วงที่ผมก็ ไล่ไล่ทำหุ้นตามที่รีquวสเอาไว้ครับเพราะ ว่าผมก็จะลิสไว้ใน Excel อยู่แล้วว่าใคร ขออะไรบ้างครับแล้วก็ไล่ๆทำนะครับดังนั้น เนี่ยผมก็เลยบอกก่อนว่าถ้าหุ้นตัวไหน เนี่ยถ้าผมเนี่ยสามารถเข้าใจได้หรือว่า ติดตามหุ้นตัวนี้แล้วก็เอามาอธิบายได้โดย ที่มั่นใจว่ามันจะไม่ผิดนะครับแต่ว่าข้อ มูลเนี่ยอาจจะไม่ตรงซะทีเดียวกับหลายๆ สื่อนะเพราะว่าแต่ละซอ์สมันก็แตกต่างกัน แต่ถ้าผมเข้าใจพอที่จะอธิบายให้เพื่อนๆพอ เก็ทได้เนี่ยผมก็จะเอามาทำนะครับแล้วก็ขอ ขอบคุณ request ที่ส่งกันมาด้วยนั่นเองนะ ครับหุ้นที่เพื่อนๆขอนะครับมันมีชื่อย่อ ว่า DCC ครับมันคือหุ้นตัวนี้ครับพาไปดู ราคาก่อนนะ ชื่อเต็มมันคือ DNAy  Ceramic ครับตัวย่อ มันก็คือ DCC ครับสปอยก็คือหุ้นตัวนี้มัน เป็นหุ้นขายตัวเซรามิครับแล้วก็เป็นร้าน ค้าปีกหรือว่าร้านค้าส่งประมาณแบบรีเทล อ่ะครับแต่ว่าขายเน้นหลักเป็นวัสดุก่อ สร้างนะครับมันก็จะมีความคล้ายกับ Home  Home เอ้ยมันมันเป็นประเภทเดียวกับ Home  Pro  Do  Home  Global ครับแต่ว่ากลุ่ม ลูกค้ามันจะออกแนว Global กับ Do  Home มากกว่าเพราะว่าทรงร้านน่ะมันไม่ใช่ร้าน แบบแนวขายของแพงขายของพรีเมี่มติดแอร์ อะไรประมาณเนี้ยนะครับก็เลยจะค่อนข้าง เข้าไปในแนวทางนั้นมากกว่าแต่ว่าประเภท ของหุ้นน่ะก็คือวัสดุก่อสร้างนะครับหุ้น ตัวเนี้ยเคยเป็นขวัญใจ VI ในอดีตครับถ้า เกิดว่าเพื่อนๆหรือว่าพี่ๆก็ตามนะที่เข้า มาเนี่ยผมเชื่อว่าน้องๆที่เข้ามาในยุค หลังโควิดอ่ะเผลอๆอาจจะไม่รู้จักหุ้นตัว นี้ครับถ้าเกิดน้องๆไม่ได้เป็นสายขุดหา หุ้นถูกหรือว่าสายหาหุ้นปันผลน่ะถ้าเกิด เป็นสายโกสอ่ะเผลอๆไม่รู้จักหุ้นตัวนี้ ครับหุ้นตัวเนี้ยนักลงทุนรุ่นเก่าๆเนี่ย เขาจะถือว่าเป็นหุ้นตัวนึงที่น่าสนใจ เพราะว่าถ้าเกิดคุณเอาในสาย Value  VI หรือว่าสายพื้นฐานน่ะมาขุดอ่ะหุ้นตัว เนี้ยอยู่ใน 1 ในลิตรแน่นอนครับเพราะว่า ตัวค่าต่างๆอ่ะค่าอัตราต่างๆค่าตัวเลข ต่างๆเนี่ยมันเข้าเค้าหมดเลยครับเป็นหุ้น ถูกราคาไม่แพงกำไรสม่ำเสมอจ่ายปันผลสวย งามค่อนข้างโอเคอะไรประมาณเนี้ยอันนี้คือ เรื่องในอดีตนะพอมาถึงภาพในปัจจุบันเนี่ย ภาพมันเปลี่ยนไปพอสมควรนะครับโอเคทีนี้มา ลองดูตัวราคาหุ้นนะครับจะเห็นว่ามันก็จะ เป็นภูเขาลูกนึงฮะลูกใหญ่ๆเลยฮะเป็นภูเขา แบบเทือกเขาล็อกกี้นะครับซึ่งเพื่อนๆไม่ ต้องแปลกใจครับเอ่อ Home  Home  Pro  Global Doโมันก็ภาพประมาณนี้ครับคืออ่า วัสดุก่อสร้างของไทยอ่ะมันไม่ภาพรวมนะภาพ รวมมันไม่ได้เติบโตเลยครับมันออกจะอิ่ม ตัวแล้วก็ทรงจะเริ่มลดด้วยซ้ำนะครับมันก็ เลยเป็นภาพว่าแบบเอมันไม่ได้มีแรงส่งอ่ะ ทำให้บริษัทพวกเนี้ยก็ต้องอาศัยฝีมือตัว เองครับอาศัยแบบโปรโมชั่นหรือว่าอาศัยแบบ การขยายไปเติบโตยังต่างประเทศนะครับเป็น สอี่ในการโกสแล้วก็เป็นสอี่ให้นักลงทุน ที่สนใจหุ้นประเภทเนี้ยสามารถถือหุ้นหรือ ว่าเทรดกันต่อไปได้ประมาณเนี้ยถ้าเกิดมอง ในระยะกลางหรือว่าระยะสั้นนะมันก็เลยทรง มาจากภาพประมาณนี้ครับระยะยาวมันก็แบบ เหมือนจะหักหัวลงนะครับซึ่งก็ต้องบอกว่า หุ้นที่มันทำได้โอเคอ่ะผมเห็นตัวเดียวตอน นี้ก็คือ Global ครับว่ามันขยายได้ไปยัง ต่างประเทศได้ค่อนข้างเห็นผลแล้วก็มี นัยยะสำคัญน่ะจริงๆตัว DNAy เนี่ยก็มี ขยายไปต่างประเทศเหมือนกันแต่ว่ามันยัง เป็นสัดส่วนน้อยครับผมให้เกดประมาณ Home Homeโอ่ะเหมือนกับเค้าพยายามอยู่นะแต่ว่า มันก็ยังไม่พีดอกออกผลน่ะประมาณนั้นนะ ครับดังนั้นเนี่ยราคาก็จะอยู่ที่ประมาณ นี้ดังนั้นเนี่ยเพื่อนๆจะเห็นเลยว่าเอิ่ม จริงๆแล้วเนี่ยตัว Dnasty เซรามิเนี่ย เค้าไม่ได้เทรดกันหรือว่าถือกันในลักษณะ ของหุ้นโกสครับเขาจะเน้นถือกันเพื่อรับ เอาปันผลมากกว่าแล้วก็ถือกันในลักษณะที่  Value มันค่อนข้างคุ้มค่าราคามันค่อน ค่อนข้างถูกอืม 2 ประเด็นเ 2 ประเด็นหลัก ๆเลยส่วนตัวเรื่องโกสเนี่ยอันนี้ผมอันนี้ ผมใช้ความเห็นส่วนตัวนะว่าอ่าถ้าจะเอาโกส จริงๆอ่ะเค้าไปซื้อดู Home  Home  Pro  Global ดีกว่าเพราะว่าอย่างแรกคือตัว บริษัทมันใหญ่กว่าแล้วมันมีสอี่ที่ชัดเจน แล้วตัวแบรนด์น่ะต้องยอมรับว่าแข็งแกร่ง กว่าครับถ้าเกิดคุณดูแค่ตัวแบรนด์น่ะ Home  Home  Pro ก็กินขาดตัว DCC อ่ะผมเชื่อ ว่าคนในกรุงเทพฯบางคนน่ะอาจจะไม่รู้จัก ด้วยซ้ำถ้าเกิดไม่ใช่เป็นนักลงทุนอย่าง นี้นะครับอืมอันเนี้ยก็คือภาพรวมโอเค ดังนั้นเนี่ยก่อนจะเข้าเนื้อหาลึกกว่า เนี้ยผมต้องบอกก่อนนะครับว่าเนื้อหาทั้ง คลิปนะครับมันไม่ได้เป็นคำแนะนำในการลง ทุนนะฮะก็ฝากเพื่อนๆใช้วิจารณญาณนะ วัตถุประสงค์ของคลิปนี้ก็คือต้องการจะ แชร์ความรู้ชวนคุยแล้วก็แชร์มุมมองเพียง เท่านั้นเลยนะครับก็ฝากดูหลายๆสื่อเพื่อ ประกอบการตัดสินใจแล้วก็ต้องรับผิดชอบ เงินของตนเองเท่านั้นเลยนะครับผมทีนี้ ครับเผื่อเพื่อนๆไม่เห็นภาพDyนตyคือ อันเนี้ยฮะคุณจะเห็นว่ามันเหมือน Glowal เหมือนดู Home ครับเห็นมั้ว่าลักษณะร้าน น่ะมันจะออกแนวแบบ Stand  alone ซะมาก กว่านะครับอันนี้ก็คือร้านในโซนภาคเหนือ ครับซึ่งรายได้หลักของเขาค้าก็จะเป็นภาค เหนือกับภาคอีสานหน้าตามันก็จะประมาณนี้ แหละนะครับอืตรงเนี้ยมีสอี่เพิ่มก็คือ เอ่อหุ้น DCC เนี่ยเค้าได้ชื่อว่าเป็น หุ้นปันผลใช่มั้ยครับแต่ว่าผมตามตัวนี้มา นานแล้วแหละพอน่าจะประมาณ 2 ปีที่แล้วอ่ะ ปี 2 ปีที่แล้วอ่ะ 1-3 ปีนี่แหละเค้ามี นโยบายลดลดอัตราเรชครับคือเหมือนว่าผมจำ ตัวเลขชัดๆไม่ได้นะปกติเนี่ยประมาณว่าแบบ เค้าเคยจ่าย Playout Rชioสมมุติ 80% น่ะ แล้วพอมาถึงช่วงนึงอ่ะเค้าปรับลดนโยบาย ครับเหลือจ่ายแค่ 40% เพราะว่าเค้าให้ เหตุผลว่าเพราะว่าณตอนนั้นเนี่ย หน้าตาของตัวสาขาเค้าเนี่ยมันไม่ได้สวย แบบนี้ครับหน้าตาสาขาเค้าก่อนหน้านั้นน่ะ มันค่อนข้างเก่าแล้วมันก็ต้องการการ  Evelop การพัฒนาเพราะว่าสถานการณ์ตอน นั้นก็คือหลายๆเจ้าก็พยายามเข้ามาแข่งดู  Home ก็เข้ามาใช่มั้ยในช่วงนั้นน่ะดู  Home  Home กำลังโกดเลยแล้วภาพรวมของ อุตสาหกรรมวัสดุก่อสร้างเนี่ยมันก็ลดลงคน ก็มีกำลังจ่ายน้อยลงแล้วไดนustตี้เนี่ย รายได้มันเริ่มตกผู้บริหารเขามองแล้วว่า ในระยะยาวเนี่ยอาจจะแพ้คู่แข่งเขาก็เลย จำเป็นต้องตัดลดเงินปันผลที่ให้ตัวนักลง ทุนน่ะเอามาทำการจัดการแล้วก็พัฒนาสาขา ต่างๆให้มันมีความทันสมัยมีความน่าเข้า มากขึ้นจะได้สู้กับผู้แข่งได้ซึ่งตรงนี้ เนี่ยมองในมุมของถ้าเราเป็นผู้บริหารนะ มันก็จำเป็นต้องทำอย่างนี้ไม่งั้นระยะยาว เนี่ยเราจะแพ้ถูกมั้ยครับแต่ทีเนี้ยมันก็ เลยกระทบกับตัวราคาหุ้นครับถ้าจำไม่ผิดผม ว่าช่วงภูเขาตรงนี้แหละภูเขาตรงเนี้ยเออ เนี่ยแหละ 23  24 ประมาณเนี้ยครับซึ่งทำ ให้ราคาหุ้นตอนนั้นน่ะมันโดนทุบครับโดน ทุบแบบหนักหนาสาหัสเลยผมว่าผมว่าอันนี้ แหละเออผมว่าเหวนี้แหละแล้วราคาหุ้นมันก็ ร่วงมาโดยตลอดครับแต่ก็ต้องบอกว่าพอ เพื่อนๆดูคลิปนี้ต่อไปอ่ะจะรู้ว่าพอช่วง หลังๆอ่ะครับเหมือนเค้าเอาเงินไปจัดการ พัฒนาสาขาหมดแล้วเนี่ยเค้าก็เอามาซื้อ หุ้นคืนครับในช่วงหลังๆก็คือช่วงปีนี้ที่ หุ้นมันค่อนข้างตกเค้าก็ซื้อหุ้นคืนมา ค่อนข้างเยอะแล้วก็มีการเพิ่มอัตราการ จ่ายเงินปันผลหรือว่า payout rชioเนี่ย ล่าสุดเนี่ยผมว่าน่าจะแตะ 80% แล้วนะก็ อาจจะเทียบเท่าของเดิมแล้วก็ได้แต่ว่า เหมือนกับว่านักลงทุนเ้าหมดใจไปแล้วครับ นะครับเพราะตอนเนี้ยนโยบายของเค้าเนี่ยก็ ยังคงเป็นนโยบายเดิมหลังจากที่เขาเปลี่ยน นะก็คือสมมุติประมาณตัวเลขที่มันต่ำกว่า นะครับเหมือนกับว่านักลงทุนน่ะเค้าขายตรง นี้แล้วเค้าแล้วก็ไปหาตัวอื่นและอาจจะไป ซื้อตัวอื่นที่มันจ่ายปันผลเหมือนกันแล้ว ก็มีนโยบายการจ่ายเงินปันผลเนี่ยที่สูง กว่าหรือว่าการจ่ายเงินปันผลที่ไม่ได้ลด เมื่อเทียบกับตัวมันเองในอดีตอะไรประมาณ เนี้ยที่มันเสถียรกว่าdyนustตyในช่วงนี้ ผ่านมาที่มันลดช่วงนึงนะครับอันนี้ก็เป็น สตอี่ของตัวราคาหุ้นที่อยากจะเสริมนะครับ อันนี้ก็คือภาพตัวของสาขาต่างๆนะครับอืม โอเคอันเนี้ยก็เป็นข้อมูลล่าสุดนะครับก็ คือ Q12025 นะครับตรงเนี้ยผมจะสรุปคร่าวเข้าให้เพราะ ว่าภาพรวมเนี่ยมันจะเป็นตัวเลขทางการเงิน เป็นส่วนใหญ่ลองค่อยๆตามกันไปนะครับทีนี้ มาลองดูตัว Performance ก่อนครับอันเนี้ย เขาจะบอกสัส่วนรายได้เลยว่า domestic ของ เขาเนี่ย 96% นะครับส่วนตัว export เนี่ย มีแค่ 4% เท่านั้นเองแปลว่ารายได้หลักของ เขาเนี่ยก็ยังคงเป็นdyนตyเซรามิในประเทศ อยู่นั่นเองครับจะเห็นว่าตัวรายได้เนี่ย อันนี้เทียบรายไตรมาสนะครับจากปีที่แล้ว เป็นต้นมาเนี่ยรายได้มันก็ลดลงตลอดแล้วก็ เพิ่งจะมาผงหัวได้ในตอนล่าสุดก็คือไตรมาส  1 ในปี 25 นะครับดังนั้นเนี่ยเนี่ยภาพ รวมก็ผมว่าก็ประมาณกลางนะก็ประมาณแบบโดน ทุบแล้วก็พยายามจะดึงกลับมาแต่ว่าภาพรวม ก็ยังคงน้อยกว่าไตรมาส 1 ในปีแรกแต่อย่าง ที่เรารู้แหละว่าช่วงเนี้ยหุ้นวัสดุก่อ สร้างนี่ค่อนข้างจะลำบากเพราะว่ากำลัง ซื้อมันค่อนข้างน้อยนะครับทั้งจากนโยบาย จ่ายของรัฐบาลแล้วก็ตัวสภาพเศรษฐกิจด้วย ตรงนี้ก็ต้องดูไปในระยะยาวนะแต่ว่าตรง เนี้ยก็เป็นข้อดีแหละว่ามันก็เริ่มผงขึ้น แต่เราก็ต้องดูต่อไปว่าไตรมาส 2 ไตรมาส 3 เนี่ยมันจะเป็นขาขึ้นหรือเปล่านั่นเอง ครับตรงนี้ยังไว้วางใจไม่ได้นะครับผมที นี้ลองดูตัว volume บ้างครับเุeก็สอด คล้องใกล้ๆกันนะครับแต่ว่าที่สังเกตก็คือ ตัว Average  P ครับก็คือตัวเส้นสีแดงที่ อยู่ข้างบนนะฮะจะเห็นว่าตัว Average  P นะถ้าเทียบปีต่อปีเนี่ยปี 25 จะสูงกว่า 24 ครับตรงเนี้ยอ่าทาง CFO เ้าบอกว่าที่เป็น อย่างเงี้ยเพราะว่าเค้าเน้นการขาย กระเบื้องแบบพิเศษจะเป็นกระเบื้องที่มี ราคาแพงมากขึ้นทำให้ตัว Average  Price เนี่ยเพิ่มขึ้นแต่ถ้าเกิดเทียบลายไตรมาส น่ะจะเห็นว่าตั้งแต่ไตรมาส 3 ปี 24 อ่ะก็ ร่วงลงมาตลอดถูกมั้ยจาก 161 เป็น 158 เป็น 157 อันนี้เขา้าบอกว่ามันเป็นปัจจัย ทางฤดูกาลแหละอืเค้าบอกว่าให้เหตุผลด้วย ความเป็น seasonal นะอือันนี้ก็ต้องฝาก เพื่อนๆเอาไปพิจารณาอีกทีนะว่าเป็นยังไง'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # subtitle = [subtitle['text'] for subtitle in subtitles]\n",
    "# with open(f\"{file_name}_subtitle.txt\", 'r') as f:\n",
    "#     contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fb231f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = file_name.split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c959756",
   "metadata": {},
   "source": [
    "## Use “Key points extraction” instead of general summary\n",
    "## Use hierarchical summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141fc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROMPTS ===\n",
    "KEY_POINTS_PROMPT = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Below is a section of a video transcript.\n",
    "Extract the key points using clear bullet points.\n",
    "Label each point with one of these: [Fact], [Idea], [Quote], [Question], [Action].\n",
    "Be precise. Keep points short and factual.\n",
    "Do not add commentary or extra text.\n",
    "\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "{chunk_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "SECTION_MERGE_PROMPT = \"\"\"\n",
    "Below are key points extracted from multiple chunks of a section.\n",
    "Merge them into a single list:\n",
    "- Remove duplicates or near-duplicates.\n",
    "- Group similar ideas if needed.\n",
    "- Keep clear bullet points.\n",
    "- Preserve labels like [Fact], [Idea], etc.\n",
    "\n",
    "Key Points:\n",
    "\\\"\\\"\\\"\n",
    "{key_points_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "FINAL_SUMMARY_PROMPT = \"\"\"\n",
    "Below are the section-level key points for the entire video.\n",
    "Write a single, well-organized summary:\n",
    "- Merge and deduplicate.\n",
    "- Organize by themes.\n",
    "- Keep [Fact], [Idea], [Quote], [Action], etc.\n",
    "- Present clearly for a slide deck or notes.\n",
    "- Do not add fluff.\n",
    "\n",
    "All Section Points:\n",
    "\\\"\\\"\\\"\n",
    "{all_sections_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# === HELPERS ===\n",
    "\n",
    "import ollama\n",
    "def call_llm(prompt: str):\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise simplify assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab32abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key points for each chunk...\n",
      "Chunk 1 done.\n",
      "Chunk 2 done.\n",
      "Chunk 3 done.\n",
      "\n",
      "Merging chunks into sections...\n",
      "Section 1 done.\n",
      "\n",
      "Merging all sections into final summary...\n",
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "Here is a single, well-organized summary:\n",
      "\n",
      "**Company Overview:**\n",
      "\n",
      "* Ceramic tile manufacturer with strong presence in Thailand\n",
      "* 96% domestic sales, with declining revenue (8,400 -> 7,700 -> 7,000) and net profit (1,700 -> 1,600 -> 1,100)\n",
      "\n",
      "**Financial Performance:**\n",
      "\n",
      "* Revenue decreased YoY since 2020 due to decline in demand\n",
      "* Net profit declined YoY since 2020 due to revenue decrease and increasing average price per unit\n",
      "\n",
      "**Industry Trends:**\n",
      "\n",
      "* Decline in ceramic tile industry demand in Thailand, attributed to consumer behavior changes, competition increase, and economic uncertainties\n",
      "\n",
      "**Management's Outlook:**\n",
      "\n",
      "* Focusing on cost reduction, operational efficiency improvement, and product diversification to drive growth\n",
      "* Gradual recovery expected over the long term\n",
      "\n",
      "**Share Price Performance:**\n",
      "\n",
      "* Fluctuated significantly (160 baht -> 130-140 baht -> 150-155 baht)\n",
      "* Stabilized in recent months\n",
      "\n",
      "**Dividend Payout:**\n",
      "\n",
      "* History of dividend payments with 80% payout ratio\n",
      "* May adjust policy to conserve cash and maintain profitability, targeting 60-70% payout ratio and 5% dividend yield\n",
      "\n",
      "**Valuation Ratios:**\n",
      "\n",
      "* PE Ratio decreased (15 -> 12)\n",
      "* Dividend Yield increased (5% -> 6% -> 8%)\n",
      "\n",
      "**Concerns:**\n",
      "\n",
      "* Declining revenue and net profit\n",
      "* Unsustainable dividend payout ratio in the long term\n",
      "\n",
      "**Analyst Views:**\n",
      "\n",
      "* Concerns about financial performance and valuation multiples (PE Ratio)\n",
      "* Attractive dividend yield despite decreasing payout ratio\n",
      "\n",
      "**Investment Strategy:**\n",
      "\n",
      "* Focus on steady growth companies with increasing assets and revenue\n",
      "* Consider valuation ratios and dividend yields when making investment decisions\n"
     ]
    }
   ],
   "source": [
    "# === PIPELINE ===\n",
    "\n",
    "chunk_key_points = []\n",
    "print(\"Extracting key points for each chunk...\")\n",
    "for i, chunk in enumerate(chunk_dict):\n",
    "    prompt = KEY_POINTS_PROMPT.format(chunk_text=chunk['text'])\n",
    "    result = call_llm(prompt)\n",
    "    print(f\"Chunk {i+1} done.\")\n",
    "    chunk_key_points.append(result)\n",
    "\n",
    "# Group into sections, e.g., 3 chunks per section\n",
    "SECTION_SIZE = 3\n",
    "section_key_points = []\n",
    "\n",
    "print(\"\\nMerging chunks into sections...\")\n",
    "for i in range(0, len(chunk_key_points), SECTION_SIZE):\n",
    "    group = chunk_key_points[i:i+SECTION_SIZE]\n",
    "    group_text = \"\\n\\n\".join(group)\n",
    "    prompt = SECTION_MERGE_PROMPT.format(key_points_text=group_text)\n",
    "    result = call_llm(prompt)\n",
    "    print(f\"Section {i//SECTION_SIZE + 1} done.\")\n",
    "    section_key_points.append(result)\n",
    "\n",
    "# Final merge\n",
    "print(\"\\nMerging all sections into final summary...\")\n",
    "all_sections_text = \"\\n\\n\".join(section_key_points)\n",
    "final_prompt = FINAL_SUMMARY_PROMPT.format(all_sections_text=all_sections_text)\n",
    "final_summary = call_llm(final_prompt)\n",
    "\n",
    "print(\"\\n✅ Final Summary:\\n\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d85f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{file_name}_summary_v2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0f143",
   "metadata": {},
   "source": [
    "## Try Map-Reduce with embeddings (advanced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
