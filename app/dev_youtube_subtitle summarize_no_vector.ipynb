{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91c6e4",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/687d132f-70f0-8003-8800-16c45376f179"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92157c8c",
   "metadata": {},
   "source": [
    "i extract subtitle from youtube video\n",
    "the video is 2hours long\n",
    "i want summarize with Key points of the video using llm model\n",
    "so i chunk subtitle every 10 minutes and input each chunk to model to summarize\n",
    "eg. 1 time summarize for 1 chunk\n",
    "and then I combine all the sumarize together\n",
    "I use this method because llm model seem to cunfuse when I input the large amount of data into it\n",
    "so I chunk it first and do all above\n",
    "is it a good method or any method suggest that better than this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc77229",
   "metadata": {},
   "source": [
    "to do\n",
    "- try chunking in token \n",
    "    - 1,000–2,000 tokens per chunk is the safe sweet spot\n",
    "    - Add 10–20% overlap to catch mid-topic cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab60894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=H8S9xg8iYuc&t=11s\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "title = soup.title.string.replace(\" - YouTube\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713eedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get video title name\n",
    "def clean_title(text):\n",
    "    return re.sub(r'[^0-9a-zA-Z\\u0E00-\\u0E7F\\.]', '', text)\n",
    "title = clean_title(text=title)\n",
    "\n",
    "# get video id\n",
    "match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", url)\n",
    "if match:\n",
    "    video_id = match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0689e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "# Fetch transcript (auto-captions or uploaded)\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['th', 'en'])\n",
    "\n",
    "# Optionally, save to file\n",
    "file_name = f'{title}_{video_id}'\n",
    "with open(f\"{file_name}_subtitle.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in transcript:\n",
    "        f.write(f\"{entry['start']:.2f}s: {entry['text']}\\n\")\n",
    "        # f.write(f\"{entry['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec099dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_subtitles(file_path):\n",
    "    subtitles = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.match(r'([0-9.]+)s:\\s(.+)', line.strip())\n",
    "            if match:\n",
    "                start_time = float(match.group(1))\n",
    "                text = match.group(2)\n",
    "                subtitles.append({'start': start_time, 'text': text})\n",
    "    return subtitles\n",
    "\n",
    "def chunk_subtitles(subtitles, chunk_size=60, overlap=20):\n",
    "    \"\"\"\n",
    "    Chunk subtitles into segments of `chunk_size` seconds with `overlap` seconds.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    max_time = subtitles[-1]['start']\n",
    "    start_time = 0\n",
    "\n",
    "    while start_time <= max_time:\n",
    "        end_time = start_time + chunk_size\n",
    "        chunk_text = []\n",
    "        for entry in subtitles:\n",
    "            if start_time <= entry['start'] < end_time:\n",
    "                chunk_text.append(entry['text'])\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                'start': start_time,\n",
    "                'end': end_time,\n",
    "                'text': ' '.join(chunk_text)\n",
    "            })\n",
    "        start_time += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "subtitles = load_subtitles(f\"{file_name}_subtitle.txt\")\n",
    "# chunk_dict = chunk_subtitles(subtitles, chunk_size=600, overlap=60)\n",
    "chunk_dict = chunk_subtitles(subtitles, chunk_size=300, overlap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e881d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0,\n",
       " 'end': 300,\n",
       " 'text': '[เพลง] ค่ะสวัสดีนักลงทุนทุกท่านนะคะก็สำหรับผล ประกอบ การของ Opportunity  Day ของบริษัทปลุก ผักพรรักแม่ในวันนี้นะคะเราได้รับเกียรติ จากผู้บริหารทั้ง 3 ท่านค่ะท่านแรกก็จะ เป็นคุณชรานะคะ CFO  CEO ค่ะสวัสดีครับ ค่ะท่านที่ 2 คุณวิทเพ็ญ CFO ค่ะสดีค่ะ ท่านที่ 3 คุณจิรายุทธ CAO ค่ะสวัสดีครับ ค่ะก็วันนี้ทั้ง 3 ท่านจะให้เกียรติเราใน การพีเซนในวันนี้นะคะสำหรับ Agenda เนี่ย เรื่องแรกก็จะเป็น Business อัปเดตนะคะ เดี๋ยวจะอัปเดตโดยคุณชลากรนะคะแล้วก็  Business  Outlook ก็เป็นคุณชลากรนะคะ ค่ะก็เดี๋ยวจะขอเริ่มที่ไซส์แรกเลยแล้ว กันค่ะ ครับผมก็ในส่วนของตัวอ่าอัปเดตนะครับ ไตรมาส 1 ของปี 68 เนี่ยนะครับเราก็ทำได้ ดีขึ้นนะครับทั้งในส่วนของตัวยอดขายกำไร สุทธิแล้วก็ตัวมาจinนะครับยอดขายเนี่ย เติบโต 33% นะครับจากปีก่อนจากการเติบโต จากทั้งของทุกแบรนด์นะครับในส่วนของตัว โอ้เองที่มีการขยายสาขาเพิ่มขึ้น 7 สาขาในปีที่แล้วแล้วก็แบรนด์ใหม่ที่เปิด ตัวในปีที่แล้วทั้งโจแล้วก็ Rap  and  R นะครับแล้วก็ตัว SSG เนี่ยอยู่ที่เติบโต อยู่ที่ 0.1%  1% ครับผมในส่วนที่ 2 เนี่ยก็คือตัวเปอร์เซ็นต์ GP อยู่ที่ 47% นะครับเติบโตจากปีก่อน 2% นะครับสาเหตุ หลักๆเลยเนี่ยก็จะมาจากการบริหารจัดการ ต้นทุนได้ดีขึ้นโดยที่เราใช้volุeในการ ต่อรองราคากับซพลerนะครับแล้วก็รวมถึงการ จัดหาวัตถุดิบที่มีคุณภาพในราคาที่เหมาะ สมแล้วก็เป็นเรื่องของการควบคุมการผลิต สินค้านะครับที่กลัวกลางซึ่งเราสามารถลด การสูญเสียตรงนั้นน่ะทำให้ดุนต่อหน่วย เนี่ยลดลงนะครับแล้วก็ในส่วนของตัวกำไร สุทธิตกโต 49% นะครับจากปีก่อนมี เปอร์เซ็นต์ Npad อยู่ที่ 9% นะครับเพิ่ม ขึ้นจากไตรมาส 1 ปี 67 นะครับซึ่งมาจาก ยอดขายนะที่สูงขึ้นนะครับจากการขยายสาขา แล้วก็เปิดตัวแบรนด์ใหม่อย่างที่ได้เรียน ไปแล้วนะครับแล้วก็เป็นการควบคุมต้นทุน ขายและการบริหารแล้วก็การจัดการในเรื่อง ของค่าใช้จ่ายอย่างมีประสิทธิภาพนะครับ แล้วก็ในส่วนของ Q1 ปี 68 นะครับปัจจุบัน เนี่ยเราเปิดในส่วนของตัวโอจูไปทั้งหมด 3 สาขาในส่วนของ Q1 ส่วนของแบรนด์โอ้จู๋ แล้วก็อื่นๆเนี่ยก็จะทยอยเปิดในส่วนของ คิวถัดไปนะครับแล้วก็เนื่องจากว่าวันที่ เราจัดทางเอ่อ earning  Call นะครับก็มี ประเด็นแล้วก็มีข้อสงสัยที่แบบสอบถามกัน เข้ามานะครับเดี๋ยวผมอาจจะขออนุญาตรบกวน ลงดีเทลตรงนี้นิดนึงในส่วนของตัว SSSG นะ ครับก็อันนี้เดี๋อาจจะรบกวนขอทางพี่แป้น ช่วยช่วยลงดีเทลตรงนี้นิดนึงครับ ค่ะสวัสดีค่ะเอ่อในส่วนของ same  stell นะคะจากวันที่ earning  claw มีการรายงาน ไปอ่ะนะคะในเรื่องของไตรมาส 2 เพื่อ อัปเดตกับนักลงทุนทุกท่านนะคะว่าส่วนของ เดือน 4 แล้วก็เดือน 5  year  to  date เป็นยังไงบ้างแล้วก็อย่างที่เรียนไปว่า มันมีการติดลบเอ่อในส่วนเนี้ยของไตรมาส 2 ประมาณ 2 ดิจิตนะคะนี้ต้องเรียนก่อนว่าขอ ชี้แจงเพิ่มเติมในข้อกังวลหรือสงสัยนะคะ อย่างแรกเลยเนี่ยก็คือเดือนเมษายนเนี่ย เอ่อเป็นเป็นตัวหลักเลยที่ทำให้เกิดตัว การอ่อนตัวลงซึ่งทุกท่านคงทราบดีอนะคะว่า ช่วงปลายมีนาคมเนี่ยมีเหตุการณ์แผ่นดิน ไหวที่เกิดขึ้นซึ่งตรงนั้นเองเนี่ยมันก็ ส่งผลให้กับลูกค้าหลายๆคนที่ทำงานอยู่ใน โซนกรุงเทพฯที่ต้องกลับไป work  from  home หรือกระทั่งเปลี่ยนที่อยู่อาศัย คอนโดนะคะตรงนั้นเองมันก็ทำให้ตัวฟิกค่อน ข้างซบเซาแล้วก็รวมไปถึงเดือนเมษายนของปี  68 เนี่ยถ้าหากกลับไปเทียบกับปี 67 ต้อง เรียนว่าในปี 67 เดือนเมษายนเป็นเดือนที่ มียอดขายอันดับ 2 เลยนะคะเพราะว่าจริงๆ มันเป็นเดือนของเทศกาลสงกรานต์นะคะรวมถึง วันจักรีที่มีวันหยุดด้วยแต่ก็อย่างที่ เรียนไปว่าอย่างวันจักรีช่วงวค.แรกมันก็ ไปติดเรื่องเหตุการณ์แผ่นดินไหวแล้วก็ถัด มากับเรื่องเศรษฐกิจชะลอตัวในช่วงเดือน 4 ที่ทุกท่านเองก็น่าจะเอ่อเห็นจากข่าวแล้ว ก็น่าจะรับทราบกันอยู่ตรงนี้แล้วอ่ะนะคะ แต่ต้องเรียนอีกว่า Same  Store  Salth ที่เราชี้แจงไปว่าติดลบ 2 ดิจิตเนี่ยมัน ไม่ใช่ Total  Sal นะคะ Same  Store  Sal คือการเติบโตของยอดขายสาขาเก่าสาขาเก่า ถ้าอย่างในสไลด์ที่เห็นเนี่ยก็คือโอ้จู๋ เพียงแบรนด์เดียวเท่านั้นที่เปิดในปี 2023 นั่นแปลว่าการคำนวณตรงเนี้ยไม่ใช่การ คำนวณของทุกสาขาโอ้จุด้วยแต่เพียง 20 กว่าสาขาเท่านั้นของโอกจุที่นำมาคำนวณนะ คะแปลว่าการอ่อนตัวเนี้ยมันคือการเทียบ ของสาขาเก่าและก็ไม่ได้รวมไปถึงแบรนด์'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6129699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # subtitle = [subtitle['text'] for subtitle in subtitles]\n",
    "# with open(f\"{file_name}_subtitle.txt\", 'r') as f:\n",
    "#     contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb231f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = file_name.split('_')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c959756",
   "metadata": {},
   "source": [
    "## Use “Key points extraction” instead of general summary\n",
    "## Use hierarchical summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141fc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROMPTS ===\n",
    "KEY_POINTS_PROMPT = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Below is a section of a video transcript.\n",
    "Extract the key points using clear bullet points.\n",
    "Label each point with one of these: [Fact], [Idea], [Quote], [Question], [Action].\n",
    "Be precise. Keep points short and factual.\n",
    "Do not add commentary or extra text.\n",
    "\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "{chunk_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "SECTION_MERGE_PROMPT = \"\"\"\n",
    "Below are key points extracted from multiple chunks of a section.\n",
    "Merge them into a single list:\n",
    "- Remove duplicates or near-duplicates.\n",
    "- Group similar ideas if needed.\n",
    "- Keep clear bullet points.\n",
    "- Preserve labels like [Fact], [Idea], etc.\n",
    "\n",
    "Key Points:\n",
    "\\\"\\\"\\\"\n",
    "{key_points_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "FINAL_SUMMARY_PROMPT = \"\"\"\n",
    "Below are the section-level key points for the entire video.\n",
    "Write a single, well-organized summary:\n",
    "- Merge and deduplicate.\n",
    "- Organize by themes.\n",
    "- Keep [Fact], [Idea], [Quote], [Action], etc.\n",
    "- Present clearly for a slide deck or notes.\n",
    "- Do not add fluff.\n",
    "\n",
    "All Section Points:\n",
    "\\\"\\\"\\\"\n",
    "{all_sections_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# === HELPERS ===\n",
    "\n",
    "import ollama\n",
    "def call_llm(prompt: str):\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise summarization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab32abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting key points for each chunk...\n",
      "Chunk 1 done.\n",
      "Chunk 2 done.\n",
      "Chunk 3 done.\n",
      "Chunk 4 done.\n",
      "Chunk 5 done.\n",
      "Chunk 6 done.\n",
      "Chunk 7 done.\n",
      "Chunk 8 done.\n",
      "\n",
      "Merging chunks into sections...\n",
      "Section 1 done.\n",
      "Section 2 done.\n",
      "Section 3 done.\n",
      "\n",
      "Merging all sections into final summary...\n",
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "Here is a well-organized summary of the section-level key points:\n",
      "\n",
      "**Company Performance**\n",
      "\n",
      "* Fact: Opportunity Day was held by บริษัทปลุก ผักพรรักแม่, with revenue growing 33% YoY in Q1 68.\n",
      "* Fact: Same Store Sales Growth (SSSG) was 0.1-1%, and same store sales declined by 2 digits in Q2 68 due to external factors.\n",
      "\n",
      "**Growth Strategy**\n",
      "\n",
      "* Idea: The company's growth was driven by new store openings and the launch of new brands, with plans to open more stores in the future.\n",
      "* Action: OKJ aims to become a leading player in the Thai food delivery market by offering unique and high-quality products.\n",
      "* Fact: The company has received positive feedback from customers and is working to improve its services and offerings.\n",
      "\n",
      "**Target Audience and Market**\n",
      "\n",
      "* Idea: OKJ's target audience is young adults aged 20-30 who are looking for a unique dining experience, with a focus on quality and brand reputation.\n",
      "* Action: OKJ plans to expand its operations by opening new branches and developing a strong online presence.\n",
      "\n",
      "**Financial Performance and Projections**\n",
      "\n",
      "* Fact: The company has a plan to increase revenue and is aiming for 20-30% growth this year, despite economic uncertainties.\n",
      "* Quote: \"Our first priority is always healthiness... that's why we focus on our employees, customers, and shareholders.\"\n",
      "\n",
      "**Other Key Points**\n",
      "\n",
      "* Idea: To improve and expand the brand, Avinage; to increase efficiency by reducing waiting time and improving processes; to expand into other locations, such as Thailand or international markets.\n",
      "* Quote: \"God profit\" - implies that there is room for improvement in terms of profit margins.\n",
      "* Question: Will the company be able to maintain its sales target and revenue growth?\n"
     ]
    }
   ],
   "source": [
    "# === PIPELINE ===\n",
    "\n",
    "chunk_key_points = []\n",
    "print(\"Extracting key points for each chunk...\")\n",
    "for i, chunk in enumerate(chunk_dict):\n",
    "    prompt = KEY_POINTS_PROMPT.format(chunk_text=chunk['text'])\n",
    "    result = call_llm(prompt)\n",
    "    print(f\"Chunk {i+1} done.\")\n",
    "    chunk_key_points.append(result)\n",
    "\n",
    "# Group into sections, e.g., 3 chunks per section\n",
    "SECTION_SIZE = 3\n",
    "section_key_points = []\n",
    "\n",
    "print(\"\\nMerging chunks into sections...\")\n",
    "for i in range(0, len(chunk_key_points), SECTION_SIZE):\n",
    "    group = chunk_key_points[i:i+SECTION_SIZE]\n",
    "    group_text = \"\\n\\n\".join(group)\n",
    "    prompt = SECTION_MERGE_PROMPT.format(key_points_text=group_text)\n",
    "    result = call_llm(prompt)\n",
    "    print(f\"Section {i//SECTION_SIZE + 1} done.\")\n",
    "    section_key_points.append(result)\n",
    "\n",
    "# Final merge\n",
    "print(\"\\nMerging all sections into final summary...\")\n",
    "all_sections_text = \"\\n\\n\".join(section_key_points)\n",
    "final_prompt = FINAL_SUMMARY_PROMPT.format(all_sections_text=all_sections_text)\n",
    "final_summary = call_llm(final_prompt)\n",
    "\n",
    "print(\"\\n✅ Final Summary:\\n\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d85f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{file_name}_summary_v2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0f143",
   "metadata": {},
   "source": [
    "## Try Map-Reduce with embeddings (advanced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
